---
title: Caret and Tidymodels
author: Ryan Johnson
date: '2019-07-29'
slug: caret-and-tidymodels
categories: [R]
tags: [caret, tidymodels]
image:
  caption: ''
  focal_point: ''
---



<pre class="r"><code>library(caret) # The caret package
library(tidymodels) # Suite of packages for tidymodeling (eg. parsnip, recipes, yardstick, etc.)
library(tidyverse) # Suite of packages for tidy data science
library(skimr) # Package for summary stats on datasets
library(cowplot) # for making multi-paneled plots
options(width = 100) # ensure skim results fit on one line</code></pre>
<p>I will be relying heavily on <a href="https://www.machinelearningplus.com/machine-learning/caret-package/">this website</a>. My goal will be to record basic vignettes for common machine learning algorithms using caretâ€¦so that I donâ€™t have to keep looking it up everytime I re-try something ğŸ˜œ. With <code>tidymodels</code> in active development, I also want to show how to implement the same <code>caret</code> code into <code>tidymodels</code>.</p>
<div id="initial-setup-and-load-data-set" class="section level1">
<h1>Initial Setup and Load Data Set</h1>
<p>The goal of this dataset is to predict which of the two brands of orange juices did the customers purchase: Citrus Hill (<code>CH</code>) or Minute Maid (<code>MM</code>).</p>
<p>The predictor variables are characteristics of the customer and the product itself. It contains 1070 rows with 18 columns. The response variable is <code>Purchase</code>.</p>
<pre class="r"><code># Create data directory
if(!dir.exists(&quot;data/raw&quot;)){
  dir.create(&quot;data/raw&quot;, recursive = TRUE)
}

# Download data set (if doesn&#39;t already exist)
if(!file.exists(&quot;data/raw/orange_juice_withmissing.csv&quot;)){
  data_url &lt;- &quot;https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv&quot;
  dest_file &lt;- &quot;data/raw/orange_juice_withmissing.csv&quot;
  download.file(url = data_url, destfile = dest_file)
}

# Read in the data set
orange &lt;- suppressMessages(suppressWarnings(read_csv(&quot;data/raw/orange_juice_withmissing.csv&quot;)))

# Summarize orange data
skim(orange)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 1070 
##  n variables: 18 
## 
## â”€â”€ Variable type:character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##  variable missing complete    n min max empty n_unique
##  Purchase       0     1070 1070   2   2     0        2
##    Store7       0     1070 1070   2   3     0        2
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete    n    mean     sd        p0    p25    p50    p75   p100     hist
##          DiscCH       2     1068 1070   0.052  0.12    0         0      0      0      0.5  â–‡â–â–â–â–â–â–â–
##          DiscMM       4     1066 1070   0.12   0.21    0         0      0      0.23   0.8  â–‡â–â–â–‚â–â–â–â–
##   ListPriceDiff       0     1070 1070   0.22   0.11    0         0.14   0.24   0.3    0.44 â–‚â–‚â–‚â–‚â–‡â–†â–â–
##         LoyalCH       5     1065 1070   0.57   0.31    1.1e-05   0.32   0.6    0.85   1    â–…â–‚â–ƒâ–ƒâ–†â–ƒâ–ƒâ–‡
##       PctDiscCH       2     1068 1070   0.027  0.062   0         0      0      0      0.25 â–‡â–â–â–â–â–â–â–
##       PctDiscMM       5     1065 1070   0.059  0.1     0         0      0      0.11   0.4  â–‡â–â–â–‚â–â–â–â–
##         PriceCH       1     1069 1070   1.87   0.1     1.69      1.79   1.86   1.99   2.09 â–‚â–…â–â–‡â–â–â–…â–
##       PriceDiff       1     1069 1070   0.15   0.27   -0.67      0      0.23   0.32   0.64 â–â–â–‚â–‚â–ƒâ–‡â–ƒâ–‚
##         PriceMM       4     1066 1070   2.09   0.13    1.69      1.99   2.09   2.18   2.29 â–â–â–â–ƒâ–â–‡â–ƒâ–‚
##     SalePriceCH       1     1069 1070   1.82   0.14    1.39      1.75   1.86   1.89   2.09 â–â–â–â–‚â–†â–‡â–…â–
##     SalePriceMM       5     1065 1070   1.96   0.25    1.19      1.69   2.09   2.13   2.29 â–â–â–ƒâ–ƒâ–â–‚â–‡â–†
##       SpecialCH       2     1068 1070   0.15   0.35    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##       SpecialMM       5     1065 1070   0.16   0.37    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##           STORE       2     1068 1070   1.63   1.43    0         0      2      3      4    â–‡â–ƒâ–â–…â–â–…â–â–ƒ
##         StoreID       1     1069 1070   3.96   2.31    1         2      3      7      7    â–ƒâ–…â–…â–ƒâ–â–â–â–‡
##  WeekofPurchase       0     1070 1070 254.38  15.56  227       240    257    268    278    â–†â–…â–…â–ƒâ–…â–‡â–†â–‡</code></pre>
<div id="convert-to-factors" class="section level3">
<h3>Convert to Factors</h3>
<p>Some columns are shown as integers, but they need to be factors (eg. <code>Purchase</code>, <code>Store7</code>)</p>
<pre class="r"><code>orange &lt;- orange %&gt;%
  mutate(Purchase = as.factor(Purchase)) %&gt;%
  mutate(Store7 = as.factor(Store7)) %&gt;%
  mutate(StoreID = as.factor(StoreID))</code></pre>
</div>
</div>
<div id="splitting-the-data-train-and-test" class="section level1">
<h1>Splitting the Data (Train and Test)</h1>
<div id="caretcreatedatapartition" class="section level3">
<h3><code>caret::createDataPartition()</code></h3>
<p>This will split the data on a based on a defined percentage that will be allocated to the training data:</p>
<pre class="r"><code>set.seed(1234)

# Step 1: Get row numbers for the training data
trainRowNumbers &lt;- createDataPartition(orange$Purchase, p = 0.75, list = FALSE)

# Step 2: Create the training  dataset
trainData &lt;- orange[trainRowNumbers,] # 75% of the data

# Step 3: Create the test dataset
testData &lt;- orange[-trainRowNumbers,]

# Store X and Y for later use.
x &lt;- trainData[, 2:18]
y &lt;- trainData$Purchase</code></pre>
</div>
<div id="rsampleinitial_split" class="section level3">
<h3><code>rsample::initial_split()</code></h3>
<p>Same thing as above but using the <code>rsample</code> package from tidymodels:</p>
<pre class="r"><code>set.seed(1234)

# Split data into training and test data
orange_split &lt;- initial_split(orange, prop = 0.75)
train_data &lt;- training(orange_split)
test_data &lt;- testing(orange_split)</code></pre>
<p>Note that despite setting the same seed, the training data sets are different. Iâ€™ll work with the <code>createDataPartition()</code> train/test data sets moving forward.</p>
<pre class="r"><code>rm(orange_split, train_data, test_data)
skim(trainData)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 803 
##  n variables: 18 
## 
## â”€â”€ Variable type:factor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##  variable missing complete   n n_unique                     top_counts ordered
##  Purchase       0      803 803        2        CH: 490, MM: 313, NA: 0   FALSE
##    Store7       0      803 803        2       No: 529, Yes: 274, NA: 0   FALSE
##   StoreID       1      802 803        5 7: 273, 2: 161, 3: 153, 1: 114   FALSE
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete   n    mean     sd        p0    p25    p50    p75   p100     hist
##          DiscCH       1      802 803   0.056  0.12    0         0      0      0      0.5  â–‡â–â–â–â–â–â–â–
##          DiscMM       4      799 803   0.12   0.21    0         0      0      0.2    0.8  â–‡â–â–â–â–â–â–â–
##   ListPriceDiff       0      803 803   0.22   0.11    0         0.14   0.24   0.3    0.44 â–‚â–‚â–‚â–â–‡â–†â–â–
##         LoyalCH       5      798 803   0.56   0.31    1.1e-05   0.32   0.59   0.85   1    â–…â–ƒâ–ƒâ–ƒâ–†â–…â–…â–‡
##       PctDiscCH       1      802 803   0.029  0.065   0         0      0      0      0.25 â–‡â–â–â–â–â–â–â–
##       PctDiscMM       4      799 803   0.058  0.1     0         0      0      0.11   0.4  â–‡â–â–â–‚â–â–â–â–
##         PriceCH       1      802 803   1.87   0.1     1.69      1.79   1.86   1.99   2.09 â–‚â–…â–â–‡â–â–â–…â–
##       PriceDiff       1      802 803   0.16   0.27   -0.67      0      0.24   0.32   0.64 â–â–â–‚â–‚â–ƒâ–‡â–…â–‚
##         PriceMM       1      802 803   2.09   0.13    1.69      2.09   2.13   2.18   2.29 â–â–â–â–ƒâ–â–‡â–ƒâ–‚
##     SalePriceCH       0      803 803   1.81   0.15    1.39      1.75   1.86   1.89   2.09 â–â–â–â–‚â–†â–‡â–…â–
##     SalePriceMM       4      799 803   1.97   0.25    1.19      1.69   2.09   2.18   2.29 â–â–â–ƒâ–ƒâ–â–‚â–‡â–†
##       SpecialCH       2      801 803   0.15   0.36    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##       SpecialMM       3      800 803   0.17   0.37    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##           STORE       2      801 803   1.61   1.43    0         0      2      3      4    â–‡â–ƒâ–â–…â–â–…â–â–ƒ
##  WeekofPurchase       0      803 803 254.51  15.39  227       240    257    268    278    â–†â–…â–…â–ƒâ–†â–‡â–‡â–‡</code></pre>
</div>
</div>
<div id="impute-missing-values" class="section level1">
<h1>Impute Missing Values</h1>
<p>This data contains a fair amount of missing data points. There are a few way to impute missing values:</p>
<ul>
<li>If categorical data, fill in the category that appears most often (i.e.Â the <em>mode</em>).</li>
<li>If numerical/continuous, fill in with the mean of the column.</li>
<li>Predict the missing values using a predictive algorithm.</li>
</ul>
<p>We will focus on the last point and use <code>k Nearest Neighbors</code> algorithm to fill in the blanks.</p>
<p>Before we continue, we should note the one missing <code>StoreID</code> row. We should expand it as a series of dummy one-hot columns:</p>
<pre class="r"><code>dmy &lt;- dummyVars(Purchase ~ ., data = trainData)
trainData_dmy &lt;- data.frame(predict(dmy, newdata = trainData))</code></pre>
<pre><code>## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$lvls): variable
## &#39;Purchase&#39; is not a factor</code></pre>
<pre class="r"><code>skim(trainData_dmy)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 803 
##  n variables: 22 
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete   n    mean     sd        p0    p25    p50    p75   p100     hist
##          DiscCH       1      802 803   0.056  0.12    0         0      0      0      0.5  â–‡â–â–â–â–â–â–â–
##          DiscMM       4      799 803   0.12   0.21    0         0      0      0.2    0.8  â–‡â–â–â–â–â–â–â–
##   ListPriceDiff       0      803 803   0.22   0.11    0         0.14   0.24   0.3    0.44 â–‚â–‚â–‚â–â–‡â–†â–â–
##         LoyalCH       5      798 803   0.56   0.31    1.1e-05   0.32   0.59   0.85   1    â–…â–ƒâ–ƒâ–ƒâ–†â–…â–…â–‡
##       PctDiscCH       1      802 803   0.029  0.065   0         0      0      0      0.25 â–‡â–â–â–â–â–â–â–
##       PctDiscMM       4      799 803   0.058  0.1     0         0      0      0.11   0.4  â–‡â–â–â–‚â–â–â–â–
##         PriceCH       1      802 803   1.87   0.1     1.69      1.79   1.86   1.99   2.09 â–‚â–…â–â–‡â–â–â–…â–
##       PriceDiff       1      802 803   0.16   0.27   -0.67      0      0.24   0.32   0.64 â–â–â–‚â–‚â–ƒâ–‡â–…â–‚
##         PriceMM       1      802 803   2.09   0.13    1.69      2.09   2.13   2.18   2.29 â–â–â–â–ƒâ–â–‡â–ƒâ–‚
##     SalePriceCH       0      803 803   1.81   0.15    1.39      1.75   1.86   1.89   2.09 â–â–â–â–‚â–†â–‡â–…â–
##     SalePriceMM       4      799 803   1.97   0.25    1.19      1.69   2.09   2.18   2.29 â–â–â–ƒâ–ƒâ–â–‚â–‡â–†
##       SpecialCH       2      801 803   0.15   0.36    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##       SpecialMM       3      800 803   0.17   0.37    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##           STORE       2      801 803   1.61   1.43    0         0      2      3      4    â–‡â–ƒâ–â–…â–â–…â–â–ƒ
##       Store7.No       0      803 803   0.66   0.47    0         0      1      1      1    â–…â–â–â–â–â–â–â–‡
##      Store7.Yes       0      803 803   0.34   0.47    0         0      0      1      1    â–‡â–â–â–â–â–â–â–…
##       StoreID.1       1      802 803   0.14   0.35    0         0      0      0      1    â–‡â–â–â–â–â–â–â–
##       StoreID.2       1      802 803   0.2    0.4     0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##       StoreID.3       1      802 803   0.19   0.39    0         0      0      0      1    â–‡â–â–â–â–â–â–â–‚
##       StoreID.4       1      802 803   0.13   0.33    0         0      0      0      1    â–‡â–â–â–â–â–â–â–
##       StoreID.7       1      802 803   0.34   0.47    0         0      0      1      1    â–‡â–â–â–â–â–â–â–…
##  WeekofPurchase       0      803 803 254.51  15.39  227       240    257    268    278    â–†â–…â–…â–ƒâ–†â–‡â–‡â–‡</code></pre>
<div id="caretpreprocess" class="section level3">
<h3><code>caret::preProcess()</code></h3>
<p>This command creates a <strong>model</strong> that can be used to impute missing values. Letâ€™s create a <code>preProcess</code> model that uses <code>knn</code>. Note that <code>preProcess</code> only works on numeric data, so by default our three factored columns (<code>Purchased</code>, <code>StoreID</code>, <code>Store7</code>) will be ignored.</p>
<pre class="r"><code># Create the knn imputation model on the training data
impute_missing_model &lt;- preProcess(trainData_dmy, method=&#39;knnImpute&#39;)
impute_missing_model</code></pre>
<pre><code>## Created from 774 samples and 22 variables
## 
## Pre-processing:
##   - centered (22)
##   - ignored (0)
##   - 5 nearest neighbor imputation (22)
##   - scaled (22)</code></pre>
<p>The above output shows the various preprocessing steps done in the process of knn imputation.</p>
<p>That is, it has centered (subtract by mean) 22 variables, ignored 0, used k=5 (considered 5 nearest neighbors) to predict the missing values and finally scaled (divide by standard deviation) 22 variables.</p>
<p>Letâ€™s now use this model to predict the missing values in trainData:</p>
<pre class="r"><code># Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData_impute &lt;- predict(impute_missing_model, newdata = trainData_dmy)

# check if any NA values exist 
if (!anyNA(trainData_impute)){
  message(&quot;All missing values have been filled in...good job!&quot;)
} else if (anyNA(trainData_impute)){
  message(&quot;Missing data still exists...try again!&quot;)
}</code></pre>
<pre><code>## All missing values have been filled in...good job!</code></pre>
</div>
<div id="recipesrecipe-prep-bake" class="section level3">
<h3><code>recipes::recipe(), prep(), bake()</code></h3>
<p>In caret, you determine what imputing algorithm youâ€™re going to use, then you run it. Within the <code>tidymodels</code> universe, you instead create a <code>recipe()</code>. Think of it exactly as the name implies, you are designing a recipe to cook somethingâ€¦but not cooking it just yet. There are things you can still tweak in your <code>recipe()</code> before actually popping it in the oven to <code>bake()</code>. Letâ€™ go ahead and add in a pre-processing step to impute our missing data with <code>knn</code> within our recipe. The recipe must start with a formula. For this example, we are going to predict <code>Purchase</code> using all predictors:</p>
<pre class="r"><code># Add forumla to recipe
orange_recipe &lt;- recipe(Purchase ~ ., data = trainData)
summary(orange_recipe)</code></pre>
<pre><code>## # A tibble: 18 x 4
##    variable       type    role      source  
##    &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 WeekofPurchase numeric predictor original
##  2 StoreID        nominal predictor original
##  3 PriceCH        numeric predictor original
##  4 PriceMM        numeric predictor original
##  5 DiscCH         numeric predictor original
##  6 DiscMM         numeric predictor original
##  7 SpecialCH      numeric predictor original
##  8 SpecialMM      numeric predictor original
##  9 LoyalCH        numeric predictor original
## 10 SalePriceMM    numeric predictor original
## 11 SalePriceCH    numeric predictor original
## 12 PriceDiff      numeric predictor original
## 13 Store7         nominal predictor original
## 14 PctDiscMM      numeric predictor original
## 15 PctDiscCH      numeric predictor original
## 16 ListPriceDiff  numeric predictor original
## 17 STORE          numeric predictor original
## 18 Purchase       nominal outcome   original</code></pre>
<p>The order in which you infer, center, scale, etc does matter (see <a href="http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/%5D">this post</a>).</p>
<ol style="list-style-type: decimal">
<li><p>Impute</p></li>
<li><p>Individual transformations for skewness and other issues</p></li>
<li><p>Discretize (if needed and if you have no other choice)</p></li>
<li><p>Create dummy variables</p></li>
<li><p>Create interactions</p></li>
<li><p>Normalization steps (center, scale, range, etc)</p></li>
<li><p>Multivariate transformation (e.g.Â PCA, spatial sign, etc)</p></li>
</ol>
<pre class="r"><code># Add imputing steps to recipe
orange_recipe_steps &lt;- orange_recipe %&gt;%
  # Add dummy variables for &quot;class&quot; variables
  step_dummy(Store7, StoreID, one_hot = TRUE) %&gt;%
  # knn impute all variables
  step_knnimpute(all_predictors()) %&gt;%
  # Center and Scale
  step_center(all_predictors()) %&gt;% 
  step_scale(all_predictors())

orange_recipe_steps</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         17
## 
## Operations:
## 
## Dummy variables from Store7, StoreID
## 5-nearest neighbor imputation for all_predictors
## Centering for all_predictors
## Scaling for all_predictors</code></pre>
<p>Now that the recipe is finished, you can see from the output exactly what is going into the recipe. <strong>Itâ€™s important to note that at this point, nothing has been imputed or transformed.</strong></p>
<p>Next, we need to see how this recipe will work on our data set. <code>prep()</code> will perform the various recipe steps on our training data and calculate the necessary statistics (eg. means and standar deviations):</p>
<pre class="r"><code>prepped_recipe &lt;- prep(orange_recipe_steps, training = trainData)</code></pre>
<pre><code>## Warning: There are new levels in a factor: NA</code></pre>
<pre class="r"><code>prepped_recipe</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         17
## 
## Training data contained 803 data points and 29 incomplete rows. 
## 
## Operations:
## 
## Dummy variables from Store7, StoreID [trained]
## 5-nearest neighbor imputation for PriceCH, PriceMM, DiscCH, DiscMM, SpecialCH, SpecialMM, ... [trained]
## Centering for WeekofPurchase, PriceCH, PriceMM, DiscCH, DiscMM, ... [trained]
## Scaling for WeekofPurchase, PriceCH, PriceMM, DiscCH, DiscMM, ... [trained]</code></pre>
<p>You can see here from the output that for each imputation/tranformation what columns are impacted. We can now</p>
<p>We can now apply our prepped recipe to the training data:</p>
<pre class="r"><code>trainData_baked &lt;- bake(prepped_recipe, new_data = trainData) # convert to the train data to the newly imputed data</code></pre>
<pre><code>## Warning: There are new levels in a factor: NA</code></pre>
<pre class="r"><code>testData_baked &lt;- bake(prepped_recipe, new_data = testData) # do the same for the test data
skim(trainData_baked)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 803 
##  n variables: 23 
## 
## â”€â”€ Variable type:factor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##  variable missing complete   n n_unique              top_counts ordered
##  Purchase       0      803 803        2 CH: 490, MM: 313, NA: 0   FALSE
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete   n         mean sd    p0    p25    p50   p75 p100     hist
##          DiscCH       0      803 803      2.5e-17  1 -0.46 -0.46  -0.46  -0.46 3.64 â–‡â–â–â–â–â–â–â–
##          DiscMM       0      803 803     -2.6e-17  1 -0.57 -0.57  -0.57   0.38 3.24 â–‡â–â–â–â–â–â–â–
##   ListPriceDiff       0      803 803     -1.3e-16  1 -2.05 -0.74   0.19   0.75 2.05 â–‚â–‚â–â–â–‡â–†â–â–
##         LoyalCH       0      803 803     -1.1e-16  1 -1.79 -0.76   0.1    0.92 1.42 â–…â–ƒâ–ƒâ–ƒâ–†â–…â–…â–‡
##       PctDiscCH       0      803 803      2.5e-17  1 -0.46 -0.46  -0.46  -0.46 3.46 â–‡â–â–â–â–â–â–â–
##       PctDiscMM       0      803 803     -7.6e-18  1 -0.58 -0.58  -0.58   0.55 3.44 â–‡â–â–â–‚â–â–â–â–
##         PriceCH       0      803 803     -2.7e-16  1 -1.75 -0.77  -0.082  1.19 2.17 â–‚â–ƒâ–‚â–‡â–â–â–†â–
##       PriceDiff       0      803 803     -3.3e-17  1 -3.04 -0.57   0.31   0.61 1.78 â–â–â–‚â–‚â–ƒâ–‡â–…â–‚
##         PriceMM       0      803 803     -4.8e-16  1 -2.99  0.013  0.31   0.69 1.51 â–â–â–â–ƒâ–â–‡â–ƒâ–‚
##     SalePriceCH       0      803 803 -3e-16        1 -2.89 -0.43   0.32   0.53 1.9  â–â–â–â–‚â–†â–‡â–…â–
##     SalePriceMM       0      803 803     -4.1e-16  1 -3.1  -1.11   0.48   0.84 1.28 â–â–â–ƒâ–ƒâ–â–‚â–‡â–†
##       SpecialCH       0      803 803     -5.1e-17  1 -0.42 -0.42  -0.42  -0.42 2.36 â–‡â–â–â–â–â–â–â–‚
##       SpecialMM       0      803 803      2.6e-18  1 -0.45 -0.45  -0.45  -0.45 2.22 â–‡â–â–â–â–â–â–â–‚
##           STORE       0      803 803      1.6e-17  1 -1.13 -1.13   0.27   0.96 1.66 â–‡â–ƒâ–â–…â–â–…â–â–ƒ
##       Store7_No       0      803 803      8.2e-17  1 -1.39 -1.39   0.72   0.72 0.72 â–…â–â–â–â–â–â–â–‡
##      Store7_Yes       0      803 803     -9.3e-18  1 -0.72 -0.72  -0.72   1.39 1.39 â–‡â–â–â–â–â–â–â–…
##      StoreID_X1       0      803 803     -1.4e-17  1 -0.41 -0.41  -0.41  -0.41 2.46 â–‡â–â–â–â–â–â–â–
##      StoreID_X2       0      803 803     -2.2e-17  1 -0.5  -0.5   -0.5   -0.5  2    â–‡â–â–â–â–â–â–â–‚
##      StoreID_X3       0      803 803      1.2e-17  1 -0.48 -0.48  -0.48  -0.48 2.06 â–‡â–â–â–â–â–â–â–‚
##      StoreID_X4       0      803 803     -1.8e-17  1 -0.38 -0.38  -0.38  -0.38 2.63 â–‡â–â–â–â–â–â–â–
##      StoreID_X7       0      803 803     -9.3e-18  1 -0.72 -0.72  -0.72   1.39 1.39 â–‡â–â–â–â–â–â–â–…
##  WeekofPurchase       0      803 803     -5.8e-16  1 -1.79 -0.94   0.16   0.88 1.53 â–†â–…â–…â–ƒâ–†â–‡â–‡â–‡</code></pre>
<p>The data is now in a form where all missing values have been imputed, all factor/nominal columns have been one-hot encoded, and is now ready for analysis!</p>
</div>
</div>
<div id="feature-analysis" class="section level1">
<h1>Feature Analysis</h1>
<p>Before you even make prediction, sometimes (if you data doesnâ€™t have 1000000â€¦ columns), you can just look to see how the features differ between the categories you are trying to predict.</p>
<div id="caretfeatureplot" class="section level3">
<h3><code>caret::featurePlot()</code></h3>
<p>Letâ€™ make some various plots to see how the preditor data is distibuted between <code>CH</code> and <code>MM</code>:</p>
<pre class="r"><code>featurePlot(x = trainData_baked[, 2:23], 
            y = trainData_baked$Purchase, 
            plot = &quot;box&quot;,
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;),
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2019-07-29-caret-and-tidymodels_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>featurePlot(x = trainData_baked[, 2:23], 
            y = trainData_baked$Purchase, 
            plot = &quot;density&quot;,
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2019-07-29-caret-and-tidymodels_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
</div>
<div id="tidymodels-feature-plots-using-tidyverse" class="section level3">
<h3>Tidymodels feature plots (using Tidyverse)</h3>
<p>I donâ€™t believe there are any quick functions built into the tidymodels packages to make quick feature plots, but this is pricicely what makes it goodâ€¦itâ€™s not too specific and plays well with other packages within the tidyverse. Here Iâ€™ll use <code>ggplot</code>, <code>purrr</code>, and <code>cowplot</code> to make similar boxplots and density plots.</p>
<pre class="r"><code>box_fun_plot = function(data, x, y) {
  ggplot(data = data, aes(x = .data[[x]],
                          y = .data[[y]],
                          fill = .data[[x]])) +
    geom_boxplot() +
    labs(title = y,
         x = x,
         y = y) +
    theme(
      legend.position = &quot;none&quot;
    )
}

# Create vector of predictors
expl &lt;- names(trainData_baked)[-1]

# Loop vector with map
expl_plots_box &lt;- map(expl, ~box_fun_plot(data = trainData_baked, x = &quot;Purchase&quot;, y = .x) )
plot_grid(plotlist = expl_plots_box)</code></pre>
<p><img src="/post/2019-07-29-caret-and-tidymodels_files/figure-html/unnamed-chunk-15-1.png" width="1440" /></p>
<pre class="r"><code>density_fun_plot = function(data, x, y) {
  ggplot(data = data, aes(x = .data[[y]],
                          fill = .data[[x]])) +
    geom_density(alpha = 0.7) +
    labs(title = y,
         x = y) +
    theme(
      legend.position = &quot;none&quot;
    )
}

# Loop vector with map
expl_plots_density &lt;- map(expl, ~density_fun_plot(data = trainData_baked, x = &quot;Purchase&quot;, y = .x) )
plot_grid(plotlist = expl_plots_density)</code></pre>
<p><img src="/post/2019-07-29-caret-and-tidymodels_files/figure-html/unnamed-chunk-16-1.png" width="1440" /></p>
</div>
</div>
<div id="training-models" class="section level1">
<h1>Training Models</h1>
<p>There are tons of machine learning algorithms out there, but to keep things simple, we are going to fit a <code>randomForest</code> model to the data. Letâ€™s first discuss how to do this in <code>caret</code></p>
<div id="carettraincontrol-train" class="section level3">
<h3><code>caret::trainControl(), train()</code></h3>
<p><code>trainControl()</code> allows us to specify how we want to train the model. Do we want to do cross validation? Reapeated cross validation? Bootstrapping? How many repetitions? All of these things get specified in <code>trainControl()</code>. With randomForest (from the ranger package), there is one parameter that needs tuning: <strong>mtry</strong> (number of variables randomly sampled as candidates at each split).</p>
<pre class="r"><code># Let&#39;s do 5 fold cross validation. This randomly splits the data into 5 parts, trains the model on 4 parts
#  and validates on the remaining 1 part. It is recommended to use repeatedcv so you can repeat this process
#  but for sake of computational time, will stick with cv.
train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5)

# Run random forest using the ranger package
set.seed(1234)
rf_fit &lt;- train(Purchase ~ .,  # Predict purchase using all predictors
               data = trainData_baked,
               method = &quot;ranger&quot;,
               trControl = train_control)
rf_fit</code></pre>
<pre><code>## Random Forest 
## 
## 803 samples
##  22 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 642, 643, 643, 642, 642 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   Accuracy   Kappa    
##    2    gini        0.7894643  0.5449255
##    2    extratrees  0.7683075  0.4961147
##   12    gini        0.8106211  0.6022994
##   12    extratrees  0.7981366  0.5726823
##   22    gini        0.8106289  0.6025827
##   22    extratrees  0.8006444  0.5784234
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 22, splitrule = gini and min.node.size = 1.</code></pre>
<p>By default, the tuning parameter mtry goes through 3 iterations (2, 12, and 22â€¦based on size of dataset). You can define your own tuning grid as well. Letâ€™s make one and expand the parameters using mtry, splitrule, and min.node.size which determines the minimum size of each terminal node in tree. A value of one make lead to overfitting/really large trees.</p>
<p>You can also give the <code>train()</code> command a preProcess argument where you can define various tools to pre-process the training data. Since we already did that previously, we donâ€™t need to hear, but itâ€™s an option you should be aware of.</p>
<pre class="r"><code>rf_grid &lt;- expand.grid(mtry = c(2, 5, 10, 20),
                      splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;),
                      min.node.size = c(1, 3, 5))

# Train new model using grid
set.seed(1234)
rf_fit_grid &lt;- train(Purchase ~ .,
                     data = trainData_baked,
                     method = &quot;ranger&quot;,
                     trControl = train_control,
                     tuneGrid = rf_grid)
rf_fit_grid</code></pre>
<pre><code>## Random Forest 
## 
## 803 samples
##  22 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 642, 643, 643, 642, 642 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   min.node.size  Accuracy   Kappa    
##    2    gini        1              0.7844876  0.5351284
##    2    gini        3              0.7894643  0.5472036
##    2    gini        5              0.7907143  0.5507238
##    2    extratrees  1              0.7707842  0.5018237
##    2    extratrees  3              0.7670730  0.4939358
##    2    extratrees  5              0.7658230  0.4907856
##    5    gini        1              0.8180745  0.6143151
##    5    gini        3              0.8218323  0.6216923
##    5    gini        5              0.8156134  0.6089515
##    5    extratrees  1              0.7969177  0.5661397
##    5    extratrees  3              0.8056134  0.5834305
##    5    extratrees  5              0.8043789  0.5810045
##   10    gini        1              0.8131134  0.6067557
##   10    gini        3              0.8180978  0.6185068
##   10    gini        5              0.8118866  0.6056586
##   10    extratrees  1              0.7969099  0.5689128
##   10    extratrees  3              0.8031366  0.5828258
##   10    extratrees  5              0.8056289  0.5880113
##   20    gini        1              0.8093944  0.6002265
##   20    gini        3              0.8143556  0.6100268
##   20    gini        5              0.8156134  0.6134386
##   20    extratrees  1              0.8018789  0.5819375
##   20    extratrees  3              0.8031599  0.5855577
##   20    extratrees  5              0.8081599  0.5961990
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 5, splitrule = gini and min.node.size = 3.</code></pre>
<p>You can see that the best model was an mtry of 10, splitrule of extratrees and a minimun node size of 5.</p>
</div>
<div id="parsnip" class="section level3">
<h3><code>parsnip</code></h3>
<p>In tidymodels, training models is done using the <code>parsnip</code> package. Letâ€™s start small and simply establish that we want to train a random forest model. We wonâ€™t specify any parameters or packages that run randomforest. Just simply â€œI want random forestâ€:</p>
<pre class="r"><code># Set model to random forest
rf_mod &lt;- rand_forest()

rf_mod</code></pre>
<pre><code>## Random Forest Model Specification (unknown)</code></pre>
<p>Similar to the <code>recipe()</code> command above, we can now start adding various parameters to our model. To see what parameters exist, you can run the <code>arg()</code> command:</p>
<pre class="r"><code>args(rand_forest)</code></pre>
<pre><code>## function (mode = &quot;unknown&quot;, mtry = NULL, trees = NULL, min_n = NULL) 
## NULL</code></pre>
<p>Just like above, you may want to tune the various parameters. You can use <code>varying()</code> as a place holder for parameters you want to tune later. We also want to tell the model we are trying to predict MM vs CH, which is a classification problem. We are also going to use <code>ranger</code> (just like above) and set the seed internally.</p>
<pre class="r"><code>rf_mod &lt;-   
  rand_forest(mtry = varying(), mode = &quot;classification&quot;) %&gt;%
  set_engine(&quot;ranger&quot;, seed = 1234)

rf_mod</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = varying()
## 
## Engine-Specific Arguments:
##   seed = 1234
## 
## Computational engine: ranger</code></pre>
<p>Letâ€™s try running the model with an mtry of 5.</p>
<pre class="r"><code>rf_mod %&gt;% 
  set_args(mtry = 5) %&gt;% 
  fit(Purchase ~ ., data = trainData_baked)</code></pre>
<pre><code>## parsnip model object
## 
## Ranger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, mtry = ~5, seed = ~1234,      num.threads = 1, verbose = FALSE, probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  500 
## Sample size:                      803 
## Number of independent variables:  22 
## Mtry:                             5 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.1318658</code></pre>
<p>Now, what if we wanted to do cross-validation? We need to go back to <code>rsample</code> package for this. This may seem like a little more work than what was presented in the <code>caret</code> part, but in a way it gives you a little more control over your data and ensures you know exactly what your doing each step of the way.</p>
<pre class="r"><code>set.seed(1234)

# Create a 5 fold cross validation dat set
cv_splits &lt;- vfold_cv(
  data = trainData_baked,
  v = 5,
  strata = &quot;Purchase&quot;
)

cv_splits</code></pre>
<pre><code>## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 2
##   splits            id   
##   &lt;named list&gt;      &lt;chr&gt;
## 1 &lt;split [642/161]&gt; Fold1
## 2 &lt;split [642/161]&gt; Fold2
## 3 &lt;split [642/161]&gt; Fold3
## 4 &lt;split [643/160]&gt; Fold4
## 5 &lt;split [643/160]&gt; Fold5</code></pre>
<p>The splits row contains how the data will be split within each fold (4/5 of the data will be in one group, 1/5 in the other). Letâ€™s look a the first fold just to see how the data will be split:</p>
<pre class="r"><code>cv_splits$splits[[1]]</code></pre>
<pre><code>## &lt;642/161/803&gt;</code></pre>
<pre class="r"><code># This will extract the 4/5ths analysis data part
cv_splits$splits[[1]] %&gt;% analysis() %&gt;% dim()</code></pre>
<pre><code>## [1] 642  23</code></pre>
<pre class="r"><code># This will extract the 1/5th assessment data part
cv_splits$splits[[1]] %&gt;% assessment() %&gt;% dim()</code></pre>
<pre><code>## [1] 161  23</code></pre>
<p>Now I need to create a function that contains my model that can be iterated over each split of the data. I also want a function that will make predictions using said model.</p>
<pre class="r"><code>fit_mod_func &lt;- function(split, spec){
  fit(object = spec,
      formula = Purchase ~ .,
      data = analysis(split))
}

predict_func &lt;- function(split, model){
  # Extract the assessment data
  assess &lt;- assessment(split)
  
  # Make prediction
  pred &lt;- predict(model, new_data = assess)
  as_tibble(cbind(assess, pred[[1]]))
}</code></pre>
<p>Then, using the <code>purrr::map()</code> function, iterate the function over each fold of the data. This will create a random forest model for each fold of the data. Store the results in a new column in <code>cv_splits</code>.</p>
<pre class="r"><code>spec_rf &lt;- rand_forest() %&gt;% 
  set_engine(&quot;ranger&quot;)

cv_splits &lt;- cv_splits %&gt;% 
  mutate(models_rf = map(.x = splits, .f = fit_mod_func, spec = spec_rf))

cv_splits</code></pre>
<pre><code>## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 3
##   splits            id    models_rf   
## * &lt;named list&gt;      &lt;chr&gt; &lt;named list&gt;
## 1 &lt;split [642/161]&gt; Fold1 &lt;fit[+]&gt;    
## 2 &lt;split [642/161]&gt; Fold2 &lt;fit[+]&gt;    
## 3 &lt;split [642/161]&gt; Fold3 &lt;fit[+]&gt;    
## 4 &lt;split [643/160]&gt; Fold4 &lt;fit[+]&gt;    
## 5 &lt;split [643/160]&gt; Fold5 &lt;fit[+]&gt;</code></pre>
<p>We can then inspect each model if you wanted:</p>
<pre class="r"><code>cv_splits$models_rf[[1]]</code></pre>
<pre><code>## parsnip model object
## 
## Ranger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      642 
## Number of independent variables:  22 
## Mtry:                             4 
## Target node size:                 1 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error:             16.36 %</code></pre>
<p>Next, add a column that contains the predictions:</p>
<pre class="r"><code>cv_splits &lt;- cv_splits %&gt;% 
  mutate(pred_rf = map2(.x = splits, .y = models_rf, .f = predict_func))

cv_splits</code></pre>
<pre><code>## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 4
##   splits            id    models_rf    pred_rf            
## * &lt;named list&gt;      &lt;chr&gt; &lt;named list&gt; &lt;named list&gt;       
## 1 &lt;split [642/161]&gt; Fold1 &lt;fit[+]&gt;     &lt;tibble [161 Ã— 24]&gt;
## 2 &lt;split [642/161]&gt; Fold2 &lt;fit[+]&gt;     &lt;tibble [161 Ã— 24]&gt;
## 3 &lt;split [642/161]&gt; Fold3 &lt;fit[+]&gt;     &lt;tibble [161 Ã— 24]&gt;
## 4 &lt;split [643/160]&gt; Fold4 &lt;fit[+]&gt;     &lt;tibble [160 Ã— 24]&gt;
## 5 &lt;split [643/160]&gt; Fold5 &lt;fit[+]&gt;     &lt;tibble [160 Ã— 24]&gt;</code></pre>
<p>To calculate performance metrics, weâ€™ll create another function:</p>
<pre class="r"><code># Will calculate accuracy of classification
perf_metrics &lt;- metric_set(accuracy)

# Create a function that will take the prediction and compare to truth
rf_metrics &lt;- function(pred_df){
  perf_metrics(
    pred_df,
    truth = Purchase,
    estimate = res # res is the column name for the predictions
  )
}

cv_splits &lt;- cv_splits %&gt;% 
  mutate(perf_rf = map(pred_rf, rf_metrics))</code></pre>
<p>Letâ€™s take a look at the first foldâ€™s accuracy:</p>
<pre class="r"><code>cv_splits$perf_rf[[1]]</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.708</code></pre>
<p>We can also average the accuracy column to get the mean accuracy for all folds in the cross-valication:</p>
<pre class="r"><code>cv_splits %&gt;% 
  unnest(perf_rf) %&gt;% 
  group_by(.metric) %&gt;% 
  summarise(
    .avg = mean(.estimate), 
    .sd = sd(.estimate)
  )</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric   .avg    .sd
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 accuracy 0.801 0.0593</code></pre>
<p>And there it is, we predicted if <code>CM</code> or <code>MM</code> with roughly 80% accuracy using random forest, but implementing it in two different environments: <code>caret</code> and <code>tidymodels</code>.</p>
</div>
</div>
